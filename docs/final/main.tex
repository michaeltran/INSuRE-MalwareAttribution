%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{IEEEtran}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\mathchardef\mhyphen="2D % Define a "math hyphen"
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{graphicx}
\graphicspath{ {./images/} }

\usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}

\title{\LARGE \bf
INSuRE: Cyber-Attack Attribution using Malware Artifacts
}

%\author{ \parbox{3 in}{\centering Huibert Kwakernaak*
%         \thanks{*Use the $\backslash$thanks command to put information here}\\
%         Faculty of Electrical Engineering, Mathematics and Computer Science\\
%         University of Twente\\
%         7500 AE Enschede, The Netherlands\\
%         {\tt\small h.kwakernaak@autsubmit.com}}
%         \hspace*{ 0.5 in}
%         \parbox{3 in}{ \centering Pradeep Misra**
%         \thanks{**The footnote marks may be inserted manually}\\
%        Department of Electrical Engineering \\
%         Wright State University\\
%         Dayton, OH 45435, USA\\
%         {\tt\small pmisra@cs.wright.edu}}
%}


\author{Michael Tran \\
        University of Houston \\
        mhtran4@uh.edu        
    \and    
        Alec Davila \\
        University of Houston \\
        andavil2@uh.edu  
    \and    
        Tu Van Nguyen \\
        University of Houston \\
        tnnguyen66@uh.edu
    \and    
        Kevin Chen \\
        University of Houston \\
        ckchen2@uh.edu}%


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

While security experts have long been conducting malware analysis, malware attribution lingers as an unsolved problem. The muddled and ephemeral nature of cyber-attack evidence -- characteristics that can be trivially influenced by attackers -- makes tracing authorship a complex task \cite{insurehub.org}. The goal of our research is to improve progress towards this front by investigating information contained within malware, such as source code or metadata, and behavioral artifacts left behind as a result of malware execution on a victim's system. In this paper, we analyze static features of malicious Windows Portable Executables (PE) from VirusTotal reports and dynamic features from Cuckoo Sandbox behavioral analysis. These features are then used to classify malware based on family. Classifiers used in testing include Naive Bayes, Support Vector Machine, and an ensemble estimator using Decision Trees as the base classifier. In addition, we propose a method to visualize and classify malware from its binary contents through a Convolutional Neural Network (CNN). Results show that the Support Vector Machine outperforms our other classifiers when considering both static and dynamic features. Visualization of malware is also proven to be effective for attribution. 

\textit{Keywords--malware, attribution, VirusTotal, Cuckoo Sandbox, visualization, Naive Bayes, Support Vector Machine, Bagging, Convolutional Neural Network}

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Malware, an abbreviation of malicious software, comes in various types such as viruses, worms, trojans, ransomware, spyware, rootkits and more -- all of which can target different platforms. Operating systems like Windows, Android, OS X, Linux and others are commonly known to be victims of malicious code. As of early 2019, weekly statistics from VirusTotal, a popular service that analyzes malicious content, indicates that approximately 33\% of detected malware are Win32 EXEs \cite{virustotal}. Windows alone, dominates almost 75\% of the global desktop operating system market share \cite{statcounter}. Because of the prevalence of PE malware, we have chosen to focus on analyzing data found in executables.

Antivirus vendors use a variety of naming schemes for file identification. While the naming format varies from vendor to vendor, most will include a family name. The family name, as described by Microsoft documentation \cite{levinec}, is a label used to group malware by common characteristics, including attribution to the same authors. For the purposes of our research, we will focus on attributing malicious executables to their corresponding malware families as a proxy for ground truth. In order to extract features from our samples, we take advantage of several malware analysis tools as described in the next section. These features are trained and tested on Naive Bayes and Support Vector Machine classifiers. In a follow-up experiment, we convert our malware into grayscale and colored images to feed into a Convolutional Neural Network (CNN) for classification. In our experiment, we obtain results for only a subsample of these images and leave the rest for future experimentation.


\section{PREVIOUS WORK}

Some teams, \cite{DBLP:journals/compsec/UcciAB19, Ye:2017:SMD:3101309.3073559, Sihwail2018ASO}, have conducted surveys detailing methods used in malware analysis. The reports detail many of the features useful for malware classification or attribution. Depending on the type of analysis performed, different features will be available. 


\subsection{Static Analysis}

In static analysis, malware characteristics can be collected without execution of the code itself. For Portable Executable (PE) files, which are native to Windows, analysts often use tools that can present tabular views of PE header information, disassemble machine language, extract printable strings, determine file hashes, etc. Some features commonly investigated as part of static analysis include file hashes, strings, op code sequences, DLL imports, API calls, and other metadata found in the PE header \cite{Ye:2017:SMD:3101309.3073559}. In malware visualization, researchers propose to view the contents of software in raw numerical form, such as binary, decimal, or hexadecimal, and convert these strings of numbers into images \cite{DBLP:conf/vizsec/NatarajKJM11}.   


\subsection{Dynamic Analysis}

In contrast to static analysis, dynamic analysis involves execution of the malicious code. While this is much harder to scale than static analysis, monitoring how a binary interacts with an infected system can lead to more insights. Analysts are often interested in recording API and system calls, processes, modifications to system files and registries, network communication, etc. \cite{DBLP:journals/compsec/UcciAB19, Gandotra2014}.  


\subsection{Hybrid Analysis}

Both static and dynamic analysis have their own limitations when conducted individually. For example, packers that compress software can be used as an obfuscation tool to obscure contents of an executable. This will often necessitate the manual efforts of a malware analyst to conduct further static analysis on machine code. Dynamic analysis is not always successful since some types of malware require a certain duration to pass or a specific event to trigger its execution. Hybrid analysis, the combination of both static and dynamic analysis, can lead to more comprehensive views of malicious programs and enable researchers to gather a larger set of features for classification \cite{Ye:2017:SMD:3101309.3073559}.    

\section{DATA PLAN}

The security landscape is constantly changing. As researchers develop new techniques to defend against hostile threats, adversaries are adapting their behavior to evade detection \cite{DBLP:journals/ieeesp/VermaKMLS15}. In this paper, we are interested in features beneficial for classification of the latest malware threats. As such, we will analyze samples collected between the dates of Feb. 2018 - Feb. 2019 from a malware repository called VirusShare \cite{virusshare.com}.

The collection of features involves a hybrid approach where static features will come from reports generated by a free malware scanning service called VirusTotal \cite{virustotal}. The binary contents of our PE files will further be converted into images, creating an additional feature space for attribution. Finally, a subsample of our malware will undergo dynamic analysis in an open source malware analysis system called Cuckoo \cite{cuckoo} where dynamic features will be gathered.

   \begin{figure}[thpb]
      \centering
      \includegraphics[scale=0.2]{flowchart}
      \caption{Hybrid analysis for malware classification}
      \label{figurelabel}
   \end{figure}

\subsection{VirusShare}

Our dataset of executables was collected from VirusShare, an online repository providing security researchers and professionals access to samples of malicious code \cite{virusshare.com}. The folders containing the malware involved in this project were uploaded to the VirusShare repository beginning Feb. 2018 and ending in Feb. 2019. The contents of the folders were filtered for PE files, and all non-Windows malware were discarded. This left us with approximately 120,000 samples. 

\subsection{VirusTotal} 

Each of our 120,000 malicious samples were scanned through VirusTotal, generating a JSON report for each sample. From our collection of JSON reports, we gathered a set of static features for our experiments. In addition to providing static features, VirusTotal displays malware labels generated by each of its 69 AntiVirus scanners employed at the time of this writing.

\subsection{AVClass}

The formatting and amount of information provided by a malware label varies from scanner to scanner. For example, Microsoft's malware labeling convention provides the type, targeted platform, family name, variant, and extra details in the form of a suffix \cite{levinec}. AVClass leverages the labels provided by the entire collection of antivirus scanners used by VirusTotal and outputs the most likely family name based on plurality voting. More details on AVClass can be found in the accompanying paper \cite{DBLP:conf/raid/SebastianRKC16}. When we test static features we prune out families containing less than 20 samples. This leaves us with approximately 95,000 samples and 248 families. 


\subsection{Cuckoo Sandbox}

We used stratified random sampling to select the executables that would undergo automated dynamic analysis in Cuckoo Sandbox. From our initial dataset of 120,000 samples belonging to 248 families, we consider only families with at least 50 samples, leaving us with 124 families. From each of these families, we randomly sample 20 malicious executables to submit for analysis by Cuckoo. This gives us 20 * 124 = 2,480 samples. Each sample is analyzed for two minutes and a behavioral report in JSON is generated. For the purposes of this experiment, we do not perform memory analysis on the system.

\subsection{Visualization} 

\begin{figure*}
  \centering
  \includegraphics[scale=0.8]{images/chart.pdf}
  \caption{Grayscale malware images with family labels}
  \label{figurelabel}
\end{figure*}

Applying a similar sampling strategy for visualization, we randomly sample 50 malicious executables from each of the 124 families containing at least 50 samples each. This gives us 50 * 124 = 6,200 malicious binaries we will consider for visualization experiments. We dump the raw bytes of each malware sample in unsigned decimal notation. We then map each decimal byte ranging from values 0-255 to a grayscale pixel with varying intensity. Black will be represented by the value 0 while white will be represented by 255. Each sample produces one image with width dependent on the executable's size. A reference table is provided below. This approach has been used in previous experiments such as \cite{DBLP:conf/vizsec/NatarajKJM11} and \cite{DBLP:journals/tii/CuiXCCWC18}. To fill the dimensions of the image with the appropriate number of pixels, some binaries required us to pad the end with byte values of "000". Some samples of our results and their corresponding family labels are shown in Figure 2.


\begin{table}[h]
\caption{File Size to Image Width for Malware Visualization}
\centering
\begin{tabular}{|c|c|}
\hline
\rowcolor[HTML]{EFEFEF} 
\textbf{File Size}    & \textbf{Image Width} \\ \hline
\textless 10 kB       & 32                   \\ \hline
10 kB - 30 kB         & 64                   \\ \hline
30 kB - 60 kB         & 128                  \\ \hline
60 kB - 100 kB        & 256                  \\ \hline
100 kB - 200 kB       & 384                  \\ \hline
200 kB - 500 kB       & 512                  \\ \hline
500 kB - 1000 kB      & 768                  \\ \hline
\textgreater{}1000 kB & 1024                 \\ \hline
\end{tabular}
\end{table}

Separate colored images are also generated with the same raw bytes. Instead of mapping one byte value to one pixel, we propose to use a sliding window of three bytes (tri-grams). The step sizes we use are one, in which case we will refer to the generated images as overlapping colored images, and three, in which case we refer to the generated images as non-overlapping colored images. Each byte will represent the value of either a Red, Green, or Blue pixel. For example, consider the following sequence of six bytes in base-10 or decimal notation:
\begin{center}
077 090 144 000 003 000.
\end{center}
From this sequence, our overlapping window would generate four pixels of values: (R: 077, G: 090, B: 144), (R: 090, G: 144, B: 000), (R: 144, G: 000, B: 003), and (R: 000, G: 003, B: 000). Our non-overlapping window would simply generate two pixels of values: (R: 077, G: 090, B: 144) and (R: 000, G: 003, B: 000).


   
\section{Action Plan}

\subsection{Feature Selection} 
The features provided by each VirusTotal scan differs from sample to sample. The same can be observed from reports generated by the Cuckoo Sandbox. For this reason, we decided to gather feature counts for both VirusTotal reports and Cuckoo reports. We then identified the features occurring with the highest frequencies across our set of approximately 96,000 samples. Based on the feature descriptions and previous works, we deemed 13 recurring features as worth investigating.
\subsection{VirusTotal Features:} 
\begin{itemize}
\item {\it TRID }: identification of file types from binary signature
% http://mark0.net/soft-trid-e.html
\item {\it PE\_RESOURCE\_LIST }: resource structure of a PE file
\item {\it EMBEDDED\_DOMAINS\_LIST }: domain names embedded in executable
\item {\it IMPORTS\_LIST }: DLLs imported and function calls used
\item {\it CONTACTED\_URLS\_LIST }: external URLs contacted
\end{itemize}
\subsection{Cuckoo Features:} 
\begin{itemize}
\item {\it SIGNATURES }: extra behavioral context
\item {\it BEHAVIOR\_CALLS }: executables run and API calls used
\item {\it BEHAVIOR\_DLL\_LOADED }: DLLs loaded
\item {\it NETWORK\_HTTP }: HTTP requests made
\item {\it NETWORK\_HOSTS }: IP addresses contacted
\item {\it STRINGS }: printable characters
\item {\it NETWORK\_UDP\_SRC }: source IP/port of UDP communication
\item {\it NETWORK\_UDP\_DST }: destination IP/port of UDP communication
\end{itemize}

\subsection{Feature Extraction}

The JSON reports were parsed, and features were extracted using the following methods:

\begin{itemize}
    \item {\it TRID}: We only take the highest probable file type from TrID and use it as a feature.
    \item {\it PE\_RESOURCE\_LIST}: We combine each resource hash and its data type using a semi-colon (resource\_hash:data\_type).
    \item {\it EMBEDDED\_DOMAINS\_LIST}: We extract each embedded URL. Only the base URLs were extracted, and instances of [http://, https://, www.] were stripped from the URL.
    \item {\it IMPORTS\_LIST}: We combine each DLL with each of its function calls using a semi-colon (import\_name:method).
    \item {\it CONTACTED\_URLS\_LIST}: We extract each contacted URL. Only the base URLs were extracted, and [http://, https://, www.] were stripped from the URL.
    \item {\it SIGNATURES}: We extract each signature from the report.
    \item {\it BEHAVIOR\_CALLS}: We extract each API call that an executable runs in the session.
    \item {\it BEHAVIOR\_DLL\_LOADED}: We extract each DLL that an executable loads.
    \item {\it NETWORK\_HTTP}: We extract each HTTP request. 
    \item {\it NETWORK\_HOSTS}: We extract each HOST request.
    \item {\it STRINGS}: We extract each string. All spaces were removed from each string.
    \item {\it NETWORK\_UDP\_SRC}: We combine each IP with its port using a semi-colon (IP:port).
    \item {\it NETWORK\_UDP\_DST}: We combine each IP with its port using a semi-colon (IP:port).
\end{itemize}

Not all features will exist for every malware report. During feature extraction, if the feature does not exist, then in most cases we will simply skip that feature. In some scenarios we will skip that particular malware sample entirely.

\subsection{Vectorization}

The types of vectorization used by each feature were determined from preliminary testing. We chose the types that were most suited to the feature and provided the highest accuracy possible. 

The following are the vectorization used for each feature:

\begin{itemize}
    \item {\it TRID}: One-hot-encoded
    \item {\it PE\_RESOURCE\_LIST}: 1, 2, 3 Grams
    \item {\it EMBEDDED\_DOMAINS\_LIST}: 1 Gram
    \item {\it IMPORTS\_LIST}: 1, 2, 3 Grams
    \item {\it CONTACTED\_URLS\_LIST}: 1, 2, 3 Grams
    \item {\it SIGNATURES}: 1, 2, 3 Grams
    \item {\it BEHAVIOR\_CALLS}: 3 Grams
    \item {\it BEHAVIOR\_DLL\_LOADED}: 1, 2, 3 Grams
    \item {\it NETWORK\_HTTP}: 1, 2, 3 Grams
    \item {\it NETWORK\_HOSTS}: 1, 2, 3 Grams
    \item {\it STRINGS}: 1, 2, 3 Grams.
    \item {\it NETWORK\_UDP\_SRC}: 3 Gram
    \item {\it NETWORK\_UDP\_DST}: 3 Gram
\end{itemize}

After all features were extracted, we employed a Chi-squared test to rank each feature. We then picked the top 50\% of ranked features as our features for classification.

\subsection{Feature Contribution Experiments} 
We use two sets of features for two different experiments. In the first experiment, we consider VirusTotal features only. In the second experiment, we consider the combination of both VirusTotal and Cuckoo features. Using methods similar to those employed in \cite{DBLP:conf/saint/SungM03}, we conduct empirical testing on each static and dynamic feature to understand how they contribute to classification results. For each of the features, we select one to leave out while using the rest as input for the following classifiers: (1) Multinomial Naive Bayes, (2) Support Vector Machine, and (3) Bagging using Decision Trees as the base estimator. We measure Accuracy, Recall, Precision, F-score, and Execution Time. The results of our experiments are shown under section V, subsections C and D.

Preliminary tests with the features NETWORK\_UDP\_SRC and NETWORK\_UDP\_DST proved they were non-useful and highly detrimental. Therefore, they were dropped from any further testing.

Execution time was shown to be too volatile between each leave-out run due to the nature of processing. Therefore, we do not consider Time when evaluating feature contributions.


\subsection{Classifiers} 

Using the features gathered from VirusTotal (static) and Cuckoo (dynamic) reports, we ran the vectorized data against three different classifiers: (1) Multinomial Naive Bayes, (2) Support Vector Machine, and (3) Bagging using Decision Trees as the base estimator. For each classifier, we tuned the hyper-parameters using exhaustive search methods.

For SVMs, we used the linear kernel, which produced the most consistent and best results. Other kernels we attempted to use were: rbf, poly, and sigmoid.

For Bagging, the hyper-parameters for the best results with a reasonable execution time were found to be: n\_estimators=1000, max\_features=0.1, and max\_samples=0.1.

We found that SVM produced the best results over Naive Bayes and Bagging. Therefore, we only show our final results using SVM.


\subsection{Visualization} 

For our visualization experiments, it was necessary to resize our images with normalized dimensions before passing them into our CNN. Our sample size was $n = 6,200$ sampled binaries. Let $m_k$ with $1 \leq k \leq 6,200$ denote a set of images corresponding to a single malware sample. Then our entire collection of image sets is represented by:

$$
M = \{m_1, m_2,...,m_n\}.
$$ 
For each malware image set $m \subset M$, we generated nine corresponding normalized images. Then, 
$$m_k = \{g_c, g_m, g_e, o_c, o_m, o_e, n_c, n_m, n_e\}$$ where:

\begin{itemize}
    \item $g_c$ = grayscale, compressed image
    \item $g_m$ = grayscale, median image
    \item $g_e$ = grayscale, expanded image
    \item $o_c$ = overlapping colored, compressed image
    \item $o_m$ = overlapping colored, median image
    \item $o_e$ = overlapping colored, expanded image
    \item $n_c$ = non-overlapping colored, compressed image
    \item $n_m$ = non-overlapping colored, median image
    \item $n_e$ = non-overlapping colored, expanded image
\end{itemize}

To derive compressed image dimensions, we took the minimum of the square root of pixel dimensions from one of the sets of grayscale, non-overlapping colored, or overlapping colored images. We then applied the floor function on the result. This gives us the dimensions of the two sides of the normalized compressed image. Let $I_g$ denote the set of grayscale image dimensions and $i$ be an arbitrary pair of grayscale image dimensions within the set. Then the compressed height, $h_c$, and compressed width, $w_c$, for all images within the set $I_g$, are derived as follows:

$$
h_c = w_c = \left \lfloor{\min_{i \in I_g} (\sqrt{i})}\right \rfloor \eqno{(1)}
$$
The same method is applied for non-overlapping colored and overlapping colored images.

To derive median image dimensions, we took the square root of the median dimension from one of the sets of grayscale, non-overlapping colored, or overlapping colored images and applied the floor function on the result. This gives us the dimensions of the two sides of the normalized median image. The median height, $h_m$, and median width, $w_m$, for all images within the set $I_g$, are derived as follows:

$$
h_m = w_m = \left \lfloor{\sqrt{m}}\right \rfloor \eqno{(2)}
$$
The same method is applied for non-overlapping colored and overlapping colored images.

To derive compressed image dimensions, we took the maximum of the square root of pixel dimensions from one of the sets of grayscale, non-overlapping colored, or overlapping colored images. We then applied the ceiling function on the result. This gives us the dimensions of the two sides of the normalized expanded image. The expanded height, $h_e$, and expanded width, $w_e$, for all images within the set $I_g$, are derived as follows:

$$
h_c = w_c = \left \lceil{\max_{i \in I_g} (\sqrt{i})}\right \rceil \eqno{(3)}
$$
The same method is applied for non-overlapping colored and overlapping colored images. The results of normalizing our sets of image dimensions are shown in Table II.

\begin{table*}[t]
\caption{Normalized Image Dimensions}
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}\textbf{Image Set}} & \textbf{Compressed (px)} & \textbf{Median (px)} & \textbf{Expanded (px)} \\ \hline
Grayscale                                                        & 56 x 56                  & 909 x 909            & 5,774 x 5,774          \\ \hline
Nonoverlapping RGB                                               & 32 x 32                  & 525 x 525            & 3,334 x 3,334            \\ \hline
Overlapping RGB                                                  & 52 x 52                  & 850 x 850            & 5,402 x 5,402            \\ \hline
\end{tabular}
\end{table*}

Using Keras \cite{chollet2015keras} and the dimensions derived in the previous sections, we used nearest neighbor interpolation to resize our original images in memory. Our image generator randomly samples, shuffles, and augments images to expand the initial training set. These images are then fed into a 2D Convolutional Neural Network with a multi-class output where output nodes represent the probability of a sample belonging to 1 of 124 malware families. We use categorial\_crossentropy for our loss function and recorded results for accuracy. Results are listed in section V under subsection E.

\section{RESULTS}

\subsection{Specs}

The following are the machine specifications used to produce the results in Tables III, IV, V, VI, VII, and VIII.

\begin{itemize}
\item Intel(R) Core(TM) i7-7700K CPU @ 4.20 GHz
\item 16 GB RAM
\end{itemize}


\subsection{Metrics}

The metrics which we use to evaluate our classifiers are defined as follows:

$$
Accuracy = \frac{TP}{TP + FP} \eqno{(4)}
$$

$$
Precision = \frac{TP}{TP + FP} \eqno{(5)}
$$

$$
Recall = \frac{TP}{TP + FN} \eqno{(6)}
$$

\newcommand\fscore{\mbox{$F$-$\mathit{score}$}}
$$
\fscore = 2 * \frac{Precision * Recall}{Precision + Recall} \eqno{(7)}
$$
where \textit{TP} = True Positives, \textit{FP} = False Positives, and \textit{FN} = False Negatives.

\subsection{VirusTotal Feature Contributions}

Our investigation of static features involved around 95,000 samples from a total of 248 malware families. The results of using all VirusTotal features for the SVM are shown in Table III. We chose one static feature to leave out and re-ran our SVM with the rest of the features as input, repeating this experiment for each of the five features. The results are shown in Table VII.  None of the static features were determined to be non-useful and therefore all five features were used for the final classifier. As such, our final results remained the same, with the exception of Time which had a negligible change. Using all static features, the final accuracy was shown to be about 85\%.

\begin{table}[h]
\caption{VirusTotal Features using SVM (95,000 Samples)}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\rowcolor[HTML]{EFEFEF} 
\textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F-score} & \textbf{Time (s)} \\ \hline
84.99             & 83.98              & 84.99           & 83.72            & 3341              \\ \hline
\end{tabular}
\end{table}

\subsection{Cuckoo Feature Contributions}

For our investigation of dynamic features, not all malware samples contained all of the eight features of interest. From our batch of 2,480 Cuckoo reports, we chose to include reports from all malware families containing at least 18 samples. This leaves us with 1,936 samples from 101 malware families. Our initial results, considering only static VirusTotal features in our SVM are shown in Table IV. The results of combining all of the Cuckoo features with the VirusTotal features for the SVM are shown in Table V. The difference between the two tables shows how hybrid analysis (combining both static and dynamic features), can improve accuracy in attribution of malware to their families. Like our previous experiments on the 95,000 sample dataset, we chose one dynamic feature to leave out and re-ran our SVM with the rest of the features as input. We repeated this experiment for each of the eight dynamic features. The results of this experiment are shown in Table VIII. NETWORK\_HTTP and NETWORK\_HOSTS were determined to be non-useful. By dropping these two features, our final accuracy increased by 0.11\% to 67.87\%. The final results are displayed in Table VI.

\begin{table}[h]
\caption{VirusTotal Features using SVM (1,936 Samples)}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\rowcolor[HTML]{EFEFEF} 
\textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F-score} & \textbf{Time (s)} \\ \hline
61.73             & 64.57              & 61.73           & 60.69            & 41              \\ \hline
\end{tabular}
\end{table}

\begin{table}[h]
\caption{VirusTotal and Cuckoo features using SVM: Initial Results (1,936 Samples)}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\rowcolor[HTML]{EFEFEF} 
\textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F-score} & \textbf{Time (s)} \\ \hline
67.87             & 69.72              & 67.87           & 66.48            & 1462              \\ \hline
\end{tabular}
\end{table}

\begin{table}[h]
\caption{VirusTotal and Cuckoo features using SVM: Final Results (1,936 Samples)}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\rowcolor[HTML]{EFEFEF} 
\textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F-score} & \textbf{Time (s)} \\ \hline
67.98             & 69.79              & 67.98           & 66.66            & 1946              \\ \hline
\end{tabular}
\end{table}


\begin{table*}[t]
\caption{VirusTotal Features using SVM: Leave-one-out Results}
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}\textbf{Dropped Feature}} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F-score} & \textbf{Time (s)} \\ \hline
TRID                                                                   & 84.81             & 83.76              & 84.81           & 83.48            & 3028              \\ \hline
PE\_RESOURCE\_LIST                                                     & 83.99             & 83.2               & 83.77           & 82.45            & 2117              \\ \hline
EMBEDDED\_DOMAINS\_LIST                                                & 84.73             & 83.71              & 83.43           & 83.43            & 3527              \\ \hline
IMPORTS\_LIST                                                          & 80.64             & 79.65              & 80.64           & 78.69            & 618               \\ \hline
CONTACTED\_URLS\_LIST                                                  & 84.78             & 83.81              & 84.78           & 83.37            & 3271              \\ \hline
\end{tabular}
\end{table*}

\begin{table*}[t]
\caption{VirusTotal and Cuckoo features using SVM: Leave-one-out Results}
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\rowcolor[HTML]{EFEFEF} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{EFEFEF}\textbf{Dropped Feature}} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F-score} & \textbf{Time (s)} \\ \hline
TRID                                                                   & 67.87             & 69.92              & 67.87           & 66.49            & 1231              \\ \hline
PE\_RESOURCE\_LIST                                                     & 67.20             & 69.27              & 67.2            & 66.00            & 1290              \\ \hline
EMBEDDED\_DOMAINS\_LIST                                                & 67.39             & 69.62              & 67.36           & 66.10            & 1346              \\ \hline
IMPORTS\_LIST                                                          & 66.99             & 69.27              & 66.99           & 65.78            & 1209              \\ \hline
CONTACTED\_URLS\_LIST                                                  & 67.72             & 69.4               & 67.72           & 66.33            & 1528              \\ \hline
SIGNATURES                                                             & 66.12             & 68.19              & 66.12           & 64.71            & 1525              \\ \hline
BEHAVIOR\_CALLS                                                        & 67.46             & 69.58              & 67.46           & 66.19            & 1137              \\ \hline
BEHAVIOR\_DLL\_LOADED                                                  & 67.56             & 69.38              & 67.56           & 66.19            & 1333              \\ \hline
NETWORK\_HTTP                                                          & 67.92             & 69.60              & 67.92           & 66.47            & 1432              \\ \hline
NETWORK\_HOSTS                                                         & 67.92             & 69.60              & 67.92           & 66.47            & 1217              \\ \hline
STRINGS                                                                & 67.39             & 69.37              & 67.36           & 66.17            & 112               \\ \hline
\end{tabular}
\end{table*}



\subsection{Classifying Malware Images}

For each of the 6,200 malware samples involved in the visualization experiment, we generated nine associated normalized images -- three of which were grayscale and six of which were colored. Due to time constraints, our classification results with the 2D CNN are limited to compressed grayscale images. For compressed grayscale images, accuracy was 97\%, proving that features from malware visualization can be used for attribution.

\section{CONCLUSIONS}

In this paper, we have conducted a hybrid analysis of malware features generated by VirusTotal reports, Cuckoo Sandbox analysis, and binary visualization. Our performance metrics evaluated how well these features trained our classifiers to attribute malicious binaries to malware families. Using a Support Vector Machine with features generated by VirusTotal and Cuckoo gave the best results, proving to have higher performance when compared to Naive Bayes and Bagging using Decision Trees. In addition, experiments were conducted to evaluate the efficacy of each static and dynamic feature for attribution. All static features were shown to useful and only a few dynamic features had detrimental contributions to our performance metrics. Finally, we performed malware image classification using a 2D Convolutional Neural Network. Our model was able to classify malware based on compressed grayscale images with high accuracy, confirming that features within malware images can be used for attribution. 

Though there has been work done in the past in clustering with various features of malware such as those found in Cuckoo reports, \cite{faridi_performance_2018, DBLP:conf/codaspy/FaridiSV19}, we believe there is room for future work in unsupervised clustering for malware images. Many papers exploring malware visualization instead focus on the task of classification. However, with an unlabeled dataset, features can be extracted from a CNN and be used as input for various clustering algorithms. Analyzing similarities among clustered malware may provide useful benefits such as signature generation or a better understanding of malware lineage \cite{DBLP:conf/codaspy/FaridiSV19}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{ACKNOWLEDGMENT}

A special thanks is given to Matthew Elder and William La Cholter of Johns Hopkins University Applied Physics Laboratory, and Rakesh Verma and Avisha Das of the Department of Computer Science at University of Houston for their consultation and support for this research.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{IEEEtran}
\bibliography{refs} % your .bib file



%% Hightlight + [Ctrl + /] to comment

% Alrabaee Saed, Saleem Noman, Preda Stere, Wang Lingyu, Debbabi Mourad. (2014) OBA2: An Onion Approach to Binary Code Authorship Attribution. In Digital Investigation, Volume 11, Supplement 1, May 2014. (pp.94-103).

% “Cyber-Attack Attribution using Malware Artifacts.” INSuRE Problem Set Repository. https://app.insurehub.org/display/IPSR/Cyber-Attack+Attribution+using+Malware+Artifacts

% “File statistics during last 7 days.” VirusTotal.

% Rascagneres, Paul, Lee Martin. “Who Wasn't Responsible for Olympic Destroyer?” Cisco's Talos Intelligence Group Blog, Cisco Talos, 26 Feb. 2018. https://blog.talosintelligence.com/2018/02/who-wasnt-responsible-for-olympic.html

% Rushabh Vyas, Xiao Luo, Nichole McFarland, Connie Justice. (2017) Investigation of Malicious Portable Executable File Detection on the Network using Supervised Learning Techniques.

% Saed Alrabaee, Paria Shirani, Mourad Debbabi, Lingyu Wang. (2017) On the Feasibility of Malware Authorship Attribution. arXiv:1701.02711 [cs.CR].

% Shalaginov Andrii, Banin Sergii, Dehghantanha Ali, Franke Katrin. Machine Learning Aided Static Malware Analysis: A Survey and Tutorial. (2018) arXiv:1808.01201v1 [cs.CR].

% Zarras Apostolis, Eckert Claudia. (2017) Finding the Needle: A Study of the PE32 Rich Header and Respective Malware Triage. In Polychronakis M., Meier M. (eds) Detection of Intrusions and Malware, and Vulnerability Assessment. DIMVA 2017. (pp. 119-138).

% VirusShare.com https://virusshare.com/


\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.
\end{document}



