# std imports
import json
import glob
import random
import pickle
import sys

# lib imports
from jsonvectorizer import JsonVectorizer, utils, vectorizers

# user imports
from json_flattener import JsonFlattener

# Globbing directories for compact json strings
data_dir = "../../../data/static_vt/compact/*/*.json"
files = glob.glob(data_dir)

# initializing flattener and doc list
flattener = JsonFlattener()
docs = []
total_files = len(files)
print("\n Reading all Json into memory")

for file in files:
    # Reading each globbed file flattening and appending to docs
    # Output of percentage is shown
    with utils.fopen(file) as f:
        for line in f:
            doc = json.loads(line)
            flat_doc = flattener.flatten_json_iterative_solution(doc)
            docs.append(flat_doc)
    done = int(50 * len(docs) / total_files)
    sys.stdout.write(
        "\r[{}{}] {}%".format(
            "█" * done, "." * (50 - done), int(100 * len(docs) / total_files)
        )
    )
    sys.stdout.flush()


# initializing JsonVectorizer
vectorizer = JsonVectorizer()

# extending vectorizer with flattened_docs
processed = 0
print("\nExtending with docs")
for doc in docs:
    vectorizer.extend(doc)
    processed += 1
    done = int(50 * processed / total_files)
    sys.stdout.write(
        "\r[{}{}] {}%".format(
            "█" * done, "." * (50 - done), int(100 * processed / total_files)
        )
    )
    sys.stdout.flush()


# print("\n Now pruning")
# pruning anything that is less that 1%
# vectorizer.prune(min_f=0.01)

# currently not working
print("\n Outputting features")
# outputting the feautures learned from the schema
for i, feature_name in enumerate(vectorizer.feature_names_):
    print("{}: {}".format(i, feature_name))


# initializing vectorizers
bool_vectorizer = {"type": "boolean", "vectorizer": vectorizers.BoolVectorizer}
# For numbers, use one-hot encoding with 10 bins
number_vectorizer = {
    "type": "number",
    "vectorizer": vectorizers.NumberVectorizer,
    "kwargs": {"n_bins": 10},
}
# For strings use tokenization, ignoring sparse (<1%) tokens
string_vectorizer = {
    "type": "string",
    "vectorizer": vectorizers.StringVectorizer,
    "kwargs": {"min_df": 0.01},
}


# fitting the vectorizer
vectorizers = [bool_vectorizer, number_vectorizer, string_vectorizer]
print("\n Now Fitting")
vectorizer.fit(vectorizers=vectorizers)


# saving the vectorizer to be used later
print("saving the vectorizer as a pickle")
pickle.dump(vectorizer, open("vectorizer_test.pkl", "wb"))
