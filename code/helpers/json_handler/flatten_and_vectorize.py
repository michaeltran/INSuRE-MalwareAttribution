# stdlib imports
import json
import glob
import os
import pickle
import sys

# package imports
from jsonvectorizer import JsonVectorizer, utils, vectorizers

# globbed path for json and pickle path
json_path = "../../../data/static_vt/compact/*/*.json"
pickle_path = "vtjs.pkl"

files = glob.glob(json_path)

total_files = len(files)
docs = []
print("\n Reading all Json into memory")
for file in files:
    with utils.fopen(file) as f:
        for line in f:
            doc = json.loads(line)
            docs.append(doc)
    done = int(50 * len(docs) / total_files)
    sys.stdout.write(
        "\r[{}{}] {}%".format(
            "â–ˆ" * done, "." * (50 - done), int(100 * len(docs) / total_files)
        )
    )
    sys.stdout.flush()

print("Beginning Vectorizer: ")
vectorizer = JsonVectorizer(tuples=True)
vectorizer.extend(doc)

print("pickling vectorizer pre-prune")
with open(pickle_path, "wb") as f:
    print("Starting pickling ")
    pickle.dump(vectorizer, f)


bool_vectorizer = {"type": "boolean", "vectorizer": vectorizers.BoolVectorizer}

# For numbers, use one-hot encoding with 10 bins
number_vectorizer = {
    "type": "number",
    "vectorizer": vectorizers.NumberVectorizer,
    "kwargs": {"n_bins": 10},
}

# For strings use tokenization, ignoring sparse (<1%) tokens
string_vectorizer = {
    "type": "string",
    "vectorizer": vectorizers.StringVectorizer,
    "kwargs": {"min_df": 0.01},
}

# Build JSON vectorizer
vectorizers = [bool_vectorizer, number_vectorizer, string_vectorizer]

vectorizer.fit(vectorizers=vectorizers)

with open("vectorizer.pkl", "wb") as f:
    pickle.dump(vectorizer, f)
