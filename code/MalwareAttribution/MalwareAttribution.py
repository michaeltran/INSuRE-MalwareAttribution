import argparse
import pandas as pd
import time
import numpy as np
import multiprocessing as mp
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

from Classifier.Classifier import Classifier, Feature

NO_PARALLELIZED_PROCESSES_DYNAMIC = 5
NO_PARALLELIZED_PROCESSES_STATIC = 5
RANDOMIZER_SEED = 1
CLF_OBJ = Classifier()
DATA_FILE_PATH = ''

def main():
    start = time.time()

    ## Get Command-line Arguments #################
    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--data', default='../../data', help='')
    opts = parser.parse_args()
    ###############################################
    global DATA_FILE_PATH
    DATA_FILE_PATH = opts.data
    InitializeOutputFile()

    global RANDOMIZER_SEED

    dynamic_training_data_dict = LoadDataDynamic(opts.data, 'dynamic_train_data.csv')
    dynamic_testing_data_dict = LoadDataDynamic(opts.data, 'dynamic_test_data.csv')
    dynamic_validation_data_dict = LoadDataDynamic(opts.data, 'dynamic_validation_data.csv')

    # Dynamic Merge Train/Test/Val Together
    Output('## DYNAMIC CROSS VALIDATION ##')
    dynamic_data_dict = {}
    for key in dynamic_training_data_dict:
        dynamic_data_dict[key] = dynamic_training_data_dict[key] + dynamic_testing_data_dict[key] + dynamic_validation_data_dict[key]
    for i in range(1, 4):
        RANDOMIZER_SEED = i
        feature_list = [
            Feature.TRID, Feature.PE_RESOURCE_LIST, Feature.EMBEDDED_DOMAINS_LIST, Feature.IMPORTS_LIST, Feature.CONTACTED_URLS_LIST,
            Feature.SIGNATURES, Feature.BEHAVIOR_CALLS, Feature.BEHAVIOR_DLL_LOADED, Feature.NETWORK_HTTP, Feature.NETWORK_HOSTS, Feature.STRINGS
        ]
        Output('Total Samples: %s' % (len(dynamic_data_dict['classification'])))
        Output('Total Classes: %s' % (len(set(dynamic_data_dict['classification']))))
        FeatureTesting(dynamic_data_dict, feature_list, 'DYNAMIC')
        Output()

    static_training_data_dict = LoadDataStatic(opts.data, 'static_train_data.csv')
    static_testing_data_dict = LoadDataStatic(opts.data, 'static_test_data.csv')
    static_validation_data_dict = LoadDataStatic(opts.data, 'static_validation_data.csv')

    # Static Merge Train/Test/Val Together
    Output('## STATIC CROSS VALIDATION ##')
    static_data_dict = {}
    for key in static_training_data_dict:
        static_data_dict[key] = static_training_data_dict[key] + static_testing_data_dict[key] + static_validation_data_dict[key]
    for i in range(1, 4):
        RANDOMIZER_SEED = i
        feature_list = [
            Feature.TRID, Feature.PE_RESOURCE_LIST, Feature.EMBEDDED_DOMAINS_LIST, Feature.IMPORTS_LIST, Feature.CONTACTED_URLS_LIST
        ]
        Output('Total Samples: %s' % (len(static_data_dict['classification'])))
        Output('Total Classes: %s' % (len(set(static_data_dict['classification']))))
        FeatureTesting(static_data_dict, feature_list, 'STATIC')
        Output()

    # DYNAMIC
    dynamic_features = [Feature.TRID, Feature.PE_RESOURCE_LIST, Feature.EMBEDDED_DOMAINS_LIST, Feature.IMPORTS_LIST, Feature.CONTACTED_URLS_LIST,
                        Feature.SIGNATURES, Feature.BEHAVIOR_CALLS, Feature.BEHAVIOR_DLL_LOADED, Feature.NETWORK_HTTP, Feature.NETWORK_HOSTS, Feature.STRINGS
                        ]
    Output('## Dynamic Testing ##')
    nb_clf = CLF_OBJ.BuildClassifierNB(dynamic_training_data_dict, dynamic_training_data_dict['classification'], dynamic_features)
    nb_predictions = nb_clf.predict(dynamic_testing_data_dict)
    Output("NB Accuracy: %0.3f" % (accuracy_score(dynamic_testing_data_dict['classification'], nb_predictions)))

    bagging_clf = CLF_OBJ.BuildClassifierBagging(dynamic_training_data_dict, dynamic_training_data_dict['classification'], dynamic_features, RANDOMIZER_SEED)
    bagging_predictions = bagging_clf.predict(dynamic_testing_data_dict)
    Output("Bagging Accuracy: %0.3f" % (accuracy_score(dynamic_testing_data_dict['classification'], bagging_predictions)))

    start = time.time()
    svm_clf = CLF_OBJ.BuildClassifierSVM(dynamic_training_data_dict, dynamic_training_data_dict['classification'], dynamic_features)
    svm_predictions = svm_clf.predict(dynamic_testing_data_dict)
    Output("SVM Accuracy: %0.3f" % (accuracy_score(dynamic_testing_data_dict['classification'], svm_predictions)))
    end = time.time()
    Output("Time Run = %fs" % (end - start))

    Output("")

    # STATIC
    static_features = [Feature.TRID, Feature.PE_RESOURCE_LIST, Feature.EMBEDDED_DOMAINS_LIST, Feature.IMPORTS_LIST, Feature.CONTACTED_URLS_LIST]
    Output('## Static Testing ##')
    nb_clf = CLF_OBJ.BuildClassifierNB(static_training_data_dict, static_training_data_dict['classification'], static_features)
    nb_predictions = nb_clf.predict(static_testing_data_dict)
    Output("NB Accuracy: %0.3f" % (accuracy_score(static_testing_data_dict['classification'], nb_predictions)))

    bagging_clf = CLF_OBJ.BuildClassifierBagging(static_training_data_dict, static_training_data_dict['classification'], static_features, RANDOMIZER_SEED)
    bagging_predictions = bagging_clf.predict(static_testing_data_dict)
    Output("Bagging Accuracy: %0.3f" % (accuracy_score(static_testing_data_dict['classification'], bagging_predictions)))

    svm_clf = CLF_OBJ.BuildClassifierSVM(static_training_data_dict, static_training_data_dict['classification'], static_features)
    svm_predictions = svm_clf.predict(static_testing_data_dict)
    Output("SVM Accuracy: %0.3f" % (accuracy_score(static_testing_data_dict['classification'], svm_predictions)))

    end = time.time()
    Output("Time Run = %fs" % (end - start))

def FeatureTesting(data, feature_list, type):
    Output('All Features Included')

    start = time.time()
    all_feature_score = CrossValidationTest(data, feature_list, type)
    end = time.time()
    Output("Cross Validation Accuracy: %0.4f (+/- %0.2f)" % (all_feature_score[:,0].mean(), all_feature_score[:,0].std()))
    Output("Cross Validation Precision: %0.4f (+/- %0.2f)" % (all_feature_score[:,1].mean(), all_feature_score[:,1].std()))
    Output("Cross Validation Recall: %0.4f (+/- %0.2f)" % (all_feature_score[:,2].mean(), all_feature_score[:,2].std()))
    Output("Cross Validation F-score: %0.4f (+/- %0.2f)" % (all_feature_score[:,3].mean(), all_feature_score[:,3].std()))
    Output("Execution Time = %fs" % (end - start))
    Output()

    all_feature_time = end - start

    best_features = []

    for i in range(len(feature_list)):
        features = [x for index, x in enumerate(feature_list) if index != i]
        Output('Leaving-out Feature: %s' % (feature_list[i].name))

        start = time.time()
        split_feature_score = CrossValidationTest(data, features, type)
        end = time.time()
        Output("Cross Validation Accuracy: %0.4f (+/- %0.2f)" % (split_feature_score[:,0].mean(), split_feature_score[:,0].std()))
        Output("Cross Validation Precision: %0.4f (+/- %0.2f)" % (split_feature_score[:,1].mean(), split_feature_score[:,1].std()))
        Output("Cross Validation Recall: %0.4f (+/- %0.2f)" % (split_feature_score[:,2].mean(), split_feature_score[:,2].std()))
        Output("Cross Validation F-score: %0.4f (+/- %0.2f)" % (split_feature_score[:,3].mean(), split_feature_score[:,3].std()))
        Output("Execution Time = %fs" % (end - start))

        accuracy_metric = all_feature_score[:,0].mean() > split_feature_score[:,0].mean()
        time_metric = all_feature_time > (end - start)

        if accuracy_metric:
            Output('Accuracy was lowered. (Feature %s was useful)' % (feature_list[i].name))
        else:
            Output('Accuracy was increased. (Feature %s was not useful)' % (feature_list[i].name))
        if time_metric:
            Output('Execution time decreased. (Good)')
        else:
            Output('Execution time increased. (Bad)')

        if accuracy_metric and time_metric:
            best_features.append(i)
        elif accuracy_metric and not time_metric:
            best_features.append(i)
        Output()

    # Final Features
    Output('Final Feature Selection')
    features = [x for index, x in enumerate(feature_list) if index in best_features]
    Output(features)
    start = time.time()
    final_score = CrossValidationTest(data, features, type)
    end = time.time()
    Output("Cross Validation Accuracy: %0.4f (+/- %0.2f)" % (final_score[:,0].mean(), final_score[:,0].std()))
    Output("Cross Validation Precision: %0.4f (+/- %0.2f)" % (final_score[:,1].mean(), final_score[:,1].std()))
    Output("Cross Validation Recall: %0.4f (+/- %0.2f)" % (final_score[:,2].mean(), final_score[:,2].std()))
    Output("Cross Validation F-score: %0.4f (+/- %0.2f)" % (final_score[:,3].mean(), final_score[:,3].std()))
    Output("Execution Time = %fs" % (end - start))

    accuracy_metric = all_feature_score[:,0].mean() > final_score[:,0].mean()
    time_metric = all_feature_time > (end - start)

    if accuracy_metric:
        Output('Accuracy was lowered. (Final feature selection was not better)')
    else:
        Output('Accuracy was increased. (Final feature selection was better)')
    if time_metric:
        Output('Execution time decreased. (Good)')
    else:
        Output('Execution time increased. (Bad)')

    Output()
    return

def CrossValidationTest(X, feature_list, type):
    kf = KFold(n_splits=10, shuffle=True, random_state=RANDOMIZER_SEED)

    cv_scores = []

    # Setup Parallelization
    if type == 'DYNAMIC':
        processes = NO_PARALLELIZED_PROCESSES_DYNAMIC
    else:
        processes = NO_PARALLELIZED_PROCESSES_STATIC
    pool = mp.Pool(processes)
    def CollectResults(result):
        cv_scores.append(result)

    for train_index, test_index in kf.split(X['index']):
        X_train = {}
        for key in X:
            X_train[key] = [X[key][i] for i in train_index]
        Y_test = {}
        for key in X:
            Y_test[key] = [X[key][i] for i in test_index]

        # Asynchronous Call
        pool.apply_async(DoCVSplitTest, args=(X_train, Y_test, feature_list), callback=CollectResults)

    # End Parallelization
    #Output('Waiting for all async calls to finish...')
    pool.close()
    pool.join()

    cv_scores = np.array(cv_scores)
    return cv_scores

def DoCVSplitTest(X_train, Y_test, feature_list):
    clf = CLF_OBJ.BuildClassifierSVM(X_train, X_train['classification'], feature_list)
    #clf = CLF_OBJ.BuildClassifierBagging(X_train, X_train['classification'], feature_list, RANDOMIZER_SEED)
    predictions = clf.predict(Y_test)
    accuracy = np.array([accuracy_score(Y_test['classification'], predictions)])
    prfs = np.array(precision_recall_fscore_support(Y_test['classification'], predictions, average='weighted', warn_for=()))
    result = np.append(accuracy, prfs)
    return result

def LoadDataStatic(data_folder_path, file_name):
    data_dict = {}
    data_dict['index'] = []
    data_dict['classification'] = []
    data_dict['embedded_domains_list'] = []
    data_dict['trid'] = []
    data_dict['pe_resource_list'] = []
    data_dict['imports_list'] = []
    data_dict['contacted_urls_list'] = []

    df = pd.read_csv(data_folder_path + '/' + file_name)
    for i in range(len(df['hash'])):
        embedded_domains_list = df['embedded_domains_list'][i]
        if embedded_domains_list != embedded_domains_list:
            embedded_domains_list = ''
        pe_resource_list = df['pe_resource_list'][i]
        if pe_resource_list != pe_resource_list:
            pe_resource_list = ''
        imports_list = df['imports_list'][i]
        if imports_list != imports_list:
            imports_list = ''
        contacted_urls_list = df['contacted_urls_list'][i]
        if contacted_urls_list != contacted_urls_list:
            contacted_urls_list = ''

        data_dict['index'].append(i)
        data_dict['classification'].append(df['classification'][i])
        data_dict['embedded_domains_list'].append(embedded_domains_list)
        data_dict['trid'].append(df['trid'][i])
        data_dict['pe_resource_list'].append(pe_resource_list)
        data_dict['imports_list'].append(imports_list)
        data_dict['contacted_urls_list'].append(contacted_urls_list)

    return data_dict

def LoadDataDynamic(data_folder_path, file_name):
    data_dict = {}
    data_dict['index'] = []
    data_dict['classification'] = []

    data_dict['signatures'] = []
    data_dict['behavior_calls'] = []
    data_dict['behavior_dll_loaded'] = []
    data_dict['network_udp_src'] = []
    data_dict['network_udp_dst'] = []
    data_dict['network_http'] = []
    data_dict['network_hosts'] = []
    data_dict['strings'] = []

    data_dict['embedded_domains_list'] = []
    data_dict['trid'] = []
    data_dict['pe_resource_list'] = []
    data_dict['imports_list'] = []
    data_dict['contacted_urls_list'] = []

    df = pd.read_csv(data_folder_path + '/' + file_name)
    for i in range(len(df['hash'])):
        signatures = df['signatures'][i]
        if signatures != signatures:
            signatures = ''
        behavior_calls = df['behavior_calls'][i]
        if behavior_calls != behavior_calls:
            behavior_calls = ''
        behavior_dll_loaded = df['behavior_dll_loaded'][i]
        if behavior_dll_loaded != behavior_dll_loaded:
            behavior_dll_loaded = ''
        network_udp_src = df['network_udp_src'][i]
        if network_udp_src != network_udp_src:
            network_udp_src = ''
        network_udp_dst = df['network_udp_dst'][i]
        if network_udp_dst != network_udp_dst:
            network_udp_dst = ''
        network_http = df['network_http'][i]
        if network_http != network_http:
            network_http = ''
        network_hosts = df['network_hosts'][i]
        if network_hosts != network_hosts:
            network_hosts = ''
        strings = df['strings'][i]
        if strings != strings:
            strings = ''

        embedded_domains_list = df['embedded_domains_list'][i]
        if embedded_domains_list != embedded_domains_list:
            embedded_domains_list = ''
        pe_resource_list = df['pe_resource_list'][i]
        if pe_resource_list != pe_resource_list:
            pe_resource_list = ''
        imports_list = df['imports_list'][i]
        if imports_list != imports_list:
            imports_list = ''
        contacted_urls_list = df['contacted_urls_list'][i]
        if contacted_urls_list != contacted_urls_list:
            contacted_urls_list = ''

        data_dict['index'].append(i)
        data_dict['classification'].append(df['classification'][i])

        data_dict['signatures'].append(signatures)
        data_dict['behavior_calls'].append(behavior_calls)
        data_dict['behavior_dll_loaded'].append(behavior_dll_loaded)
        data_dict['network_udp_src'].append(network_udp_src)
        data_dict['network_udp_dst'].append(network_udp_dst)
        data_dict['network_http'].append(network_http)
        data_dict['network_hosts'].append(network_http)
        data_dict['strings'].append(strings)

        data_dict['embedded_domains_list'].append(embedded_domains_list)
        data_dict['trid'].append(df['trid'][i])
        data_dict['pe_resource_list'].append(pe_resource_list)
        data_dict['imports_list'].append(imports_list)
        data_dict['contacted_urls_list'].append(contacted_urls_list)

    return data_dict

def InitializeOutputFile():
    with open(DATA_FILE_PATH + '/' + 'output.txt', 'w') as file:
        file.write('')

def Output(text=''):
    print(text)
    with open(DATA_FILE_PATH + '/' + 'output.txt', 'a') as file:
        file.write(str(text) + '\n')

if __name__ == '__main__':
    main()