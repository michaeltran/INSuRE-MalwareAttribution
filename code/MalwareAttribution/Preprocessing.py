import os
import argparse
import re
import json
import random
import time
import multiprocessing as mp

from pandas import DataFrame

NO_PARALLELIZED_PROCESSES = 7

def GetAllStaticData(static_data_path, classification_dict):
    static_malware_data = {}

    # Setup Static Data Parallelization
    print('Getting Static Data...')
    #pool = mp.Pool(int(mp.cpu_count() / 2))
    pool = mp.Pool(NO_PARALLELIZED_PROCESSES)
    def CollectStaticResults(result):
        static_malware_data[result[0]] = result[1]

    # Get Malware Static Data (VirusTotal)
    for folder_name in os.listdir(static_data_path):
        # Check if folder_name is a folder
        if not os.path.isdir(static_data_path + '/' + folder_name):
            continue
        for file_name in os.listdir(static_data_path + '/' + folder_name):
            hash = re.findall("(.*).json", file_name)[0]
            # Check if hash is labled
            if hash not in classification_dict:
                continue
            classification = classification_dict[hash]
            # Asynchronous Call
            pool.apply_async(GetStaticData, args=(static_data_path, folder_name, file_name, hash, classification), callback=CollectStaticResults)

    # End Static Data Parallelization
    print('Waiting for all async calls to finish...')
    pool.close()
    pool.join()

    return static_malware_data

def GetStaticData(static_data_path, folder_name, file_name, hash, classification):
    with open(static_data_path + '/' + folder_name + '/' + file_name) as file:
        data = json.load(file)

        embedded_domains_list = []
        trid_data = ''
        pe_resource_list = []
        imports_list = []
        contacted_urls_list = []
        type = ''
        tags_list = []

        ## Additional Info
        if 'embedded_domains' in data['additional_info']:
            for embedded_domain in data['additional_info']['embedded_domains']:
                x = re.findall("^.+?[^\/:](?=[?\/]|$)", embedded_domain)[0]
                x = x.replace("http://", "")
                x = x.replace("https://", "")
                x = x.replace("www.", "")
                embedded_domains_list.append(x)
        # Get the top ranked trid description
        if 'trid' in data['additional_info']:
            trid_data = re.findall("(.*)\(.*%\)", data['additional_info']['trid'].split('\n')[0])[0]
            trid_data = trid_data.strip()
        # Get pe_resource_list
        if 'pe-resource-list' in data['additional_info']:
            for pe_resource in data['additional_info']['pe-resource-list']:
                pe_resource_list.append(pe_resource + ':' + data['additional_info']['pe-resource-list'][pe_resource].replace(" ", ""))
        # Get imports
        if 'imports' in data['additional_info']:
            for import_name in data['additional_info']['imports']:
                imports_list.append(import_name)
                for method in data['additional_info']['imports'][import_name]:
                    imports_list.append(import_name + ':' + method.replace(" ", ""))
        if 'contacted_urls' in data['additional_info']:
            for url in data['additional_info']['contacted_urls']:
                x = re.findall("^.+?[^\/:](?=[?\/]|$)", url)[0]
                x = x.replace("http://", "")
                x = x.replace("https://", "")
                x = x.replace("www.", "")
                contacted_urls_list.append(x)

        ## S
        if 'type' in data:
            type = data['type']
        if 'tags' in data:
            for tag in data['tags']:
                tags_list.append(tag)

        temp = {}
        temp['hash'] = hash
        temp['classification'] = classification
        temp['embedded_domains_list'] = '``'.join(embedded_domains_list)
        temp['trid'] = trid_data
        temp['pe_resource_list'] = ' '.join(pe_resource_list)
        temp['imports_list'] = ' '.join(imports_list)
        temp['contacted_urls_list'] = '``'.join(contacted_urls_list)
        temp['type'] = type
        temp['tags_list'] = ' '.join(tags_list)
    return (hash, temp)

def GetAllDynamicData(dynamic_data_path, classification_dict):
    dynamic_malware_data = {}

    # Setup Dynamic Data Parallelization
    print('Getting Dynamic Data...')
    pool = mp.Pool(NO_PARALLELIZED_PROCESSES)
    def CollectDynamicResults(result):
        if result is not None:
            dynamic_malware_data[result[0]] = result[1]

    # Get Malware Dynamic Data (Cuckoo)
    for folder_name in os.listdir(dynamic_data_path):
        if not os.path.isdir(dynamic_data_path + '/' + folder_name):
            continue
        for file_name in os.listdir(dynamic_data_path + '/' + folder_name):
            hash = re.findall("(.*).json", file_name)[0]
            if hash not in classification_dict:
                continue
            classification = classification_dict[hash]
            # Asynchronous Call
            pool.apply_async(GetDynamicData, args=(dynamic_data_path, folder_name, file_name, hash, classification), callback=CollectDynamicResults)

    # End Dynamic Data Parallelization
    print('Waiting for all async calls to finish...')
    pool.close()
    pool.join()

    return dynamic_malware_data

def GetDynamicData(dynamic_data_path, folder_name, file_name, hash, classification):
    try:
        with open(dynamic_data_path + '/' + folder_name + '/' + file_name, 'r') as file:
            data = json.load(file)
            signatures = []
            behavior_calls_api = []
            behavior_dll_loaded = []
            network_udp_src = []
            network_udp_dst = []
            network_http = []
            network_hosts = []
            strings = []

            # Get Signature Data
            for signature in data['signatures']:
                signatures.append(signature['name'])

            # Get System Call Data
            if (len(data['behavior']['processes']) > 1):
                for process in data['behavior']['processes']:
                        # TO DO: It is very possible for timestamps to be the same, so we should handle this somehow
                        behavior_calls = process['calls']
                        #behavior_calls = sorted(behavior_calls, key = lambda i: i['time'])
                        for calls in behavior_calls:
                            behavior_calls_api.append(calls['api'])

            # Get DLLs Loaded
            if 'summary' in data['behavior']:
                if 'dll_loaded' in data['behavior']['summary']:
                    dlls_loaded = data['behavior']['summary']['dll_loaded']
                    for dlls in dlls_loaded:
                        base_dlls = os.path.basename(dlls)
                        behavior_dll_loaded.append(base_dlls)

            # Get Network Data
            network_udp_data = data['network']['udp']
            #network_udp_data = sorted(network_udp_data, key = lambda i: i['time'])
            for udp in network_udp_data:
                network_udp_src.append(udp['src'] + ':' + str(udp['sport']))
                network_udp_dst.append(udp['dst'] + ':' + str(udp['dport']))
            
            # Get HTTP Data
            network_http_data = data['network']['http']
            for http in network_http_data:
                x = re.findall("^.+?[^\/:](?=[?\/]|$)", str(http['host']))[0]
                x = x.replace("http://", "")
                x = x.replace("https://", "")
                x = x.replace("www.", "")
                network_http.append(x)

            # Get Host Data
            network_host_data = data['network']['hosts']
            for host in network_host_data:
                network_hosts.append(host)

            # Get Strings
            for string_data in data['strings']:
                string_data = "".join(string_data.split())
                strings.append(string_data)

            temp = {}
            temp['classification'] = classification
            temp['signatures'] = ' '.join(signatures)
            temp['behavior_calls'] = ' '.join(behavior_calls_api)
            temp['behavior_dll_loaded'] = ' '.join(behavior_dll_loaded)
            temp['network_udp_src'] = ' '.join(network_udp_src)
            temp['network_udp_dst'] = ' '.join(network_udp_dst)
            temp['network_http'] = ' '.join(network_http)
            temp['network_hosts'] = ' '.join(network_hosts)
            temp['strings'] = ' '.join(strings)
    except Exception as e:
        print('Skipped file: %s/%s due to error: %s' % (folder_name, file_name, e))
        return
    return (hash, temp)

def PreProcessing(data_folder_path):
    start = time.time()

    static_label_path = data_folder_path + '/static_vt/labels'
    static_data_path = data_folder_path + '/static_vt/compact'
    dynamic_data_path = data_folder_path + '/dynamic_cuckoo/reports'

    # Check if data folders exists
    if not os.path.exists(static_label_path):
        print('Could not find labels: %s' % (os.path.abspath(static_label_path)))
        return
    if not os.path.exists(static_data_path):
        print('Could not find static data: %s' % (os.path.abspath(static_data_path)))
        return
    if not os.path.exists(dynamic_data_path):
        print('Could not find dynamic data: %s' % (os.path.abspath(dynamic_data_path)))
        return

    static_malware_data = {}
    dynamic_malware_data ={}

    # Get all Malware Hashes and Labels
    print('Getting Malware Hashes and Labels...')
    classification_dict = {}
    for folder_name in os.listdir(static_label_path):
        # Check if folder_name is a folder
        if not os.path.isdir(static_label_path + '/' + folder_name):
            continue
        with open(static_label_path + '/' + folder_name + '/' + folder_name + '_labels.txt', 'r') as file:
            for line in file:
                x = re.findall("(.*)\\t(.*)\\n", line)
                hash = x[0][0]
                label = x[0][1]
                # Check if label is Singleton (Means there is only one type of sample)
                if label.startswith('SINGLETON'):
                    continue
                classification_dict[hash] = label

    static_malware_data = GetAllStaticData(static_data_path, classification_dict)
    dynamic_malware_data = GetAllDynamicData(dynamic_data_path, classification_dict)

    malware_data = static_malware_data

    # Save all Malware Samples
    all_malware_samples = []
    for key in malware_data:
        all_malware_samples.append(malware_data[key])
    print('Saving to static_data.csv...')
    WriteToExcelStatic(data_folder_path + '/static_data.csv', all_malware_samples)

    training_malware_data, testing_malware_data, validation_malware_data = SplitData(malware_data, 20, 100000)

    print('Saving static_train_data.csv...')
    WriteToExcelStatic(data_folder_path + '/static_train_data.csv', training_malware_data)
    print('Saving static_test_data.csv...')
    WriteToExcelStatic(data_folder_path + '/static_test_data.csv', testing_malware_data)
    print('Saving static_validation_data.csv...')
    WriteToExcelStatic(data_folder_path + '/static_validation_data.csv', validation_malware_data)

    malware_data = {}
    for key in dynamic_malware_data:
        if key in static_malware_data:
            data = dynamic_malware_data[key]
            for data_type in static_malware_data[key]:
                data[data_type] = static_malware_data[key][data_type]
            malware_data[key] = data

    # Save all Malware Samples
    all_malware_samples = []
    for key in malware_data:
        all_malware_samples.append(malware_data[key])
    print('Saving to dynamic_data.csv...')
    WriteToExcelDynamic(data_folder_path + '/dynamic_data.csv', all_malware_samples)

    training_malware_data, testing_malware_data, validation_malware_data = SplitData(malware_data, 20, 100000)

    print('Saving dynamic_train_data.csv...')
    WriteToExcelDynamic(data_folder_path + '/dynamic_train_data.csv', training_malware_data)
    print('Saving dynamic_test_data.csv...')
    WriteToExcelDynamic(data_folder_path + '/dynamic_test_data.csv', testing_malware_data)
    print('Saving dynamic_validation_data.csv...')
    WriteToExcelDynamic(data_folder_path + '/dynamic_validation_data.csv', validation_malware_data)

    end = time.time()
    print("Time Run = %fs" % (end - start))

def WriteToExcelStatic(path, malware_data):
    df = DataFrame(malware_data, 
                   columns=['classification', 'hash', 
                            'embedded_domains_list', 'trid', 'pe_resource_list', 'imports_list', 'contacted_urls_list', 'type', 'tags_list'])

    ## Checking for large data
    #for i in range(len(malware_data)):
    #    for key in malware_data[i]:
    #        if len(malware_data[i][key]) > 32000:
    #            print(key + ' : ' + str(len(malware_data[i][key])))

    df.to_csv(path, index=None, header=True)

    #with xlsxwriter.Workbook(path, {'strings_to_urls': False}) as workbook:
    #    worksheet = workbook.add_worksheet();
    #    row = 0;
    #    col = 0;
    #    # Header Row
    #    worksheet.write(row, col, 'classification'); col += 1;
    #    worksheet.write(row, col, 'hash'); col += 1;
    #    worksheet.write(row, col, 'embedded_domains_list'); col += 1;
    #    worksheet.write(row, col, 'trid'); col += 1;
    #    worksheet.write(row, col, 'pe_resource_list'); col += 1;
    #    worksheet.write(row, col, 'imports_list'); col += 1;
    #    worksheet.write(row, col, 'contacted_urls_list'); col += 1;
    #    worksheet.write(row, col, 'type'); col += 1;
    #    worksheet.write(row, col, 'tags_list'); col += 1;
    #    row += 1;
    #    # Data Rows
    #    for key in range(len(malware_data)):
    #        col = 0;
    #        worksheet.write(row, col, malware_data[key]['classification']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['hash']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['embedded_domains_list']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['trid']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['pe_resource_list']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['imports_list']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['contacted_urls_list']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['type']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['tags_list']); col += 1;
    #        row += 1;
    return

def WriteToExcelDynamic(path, malware_data):
    df = DataFrame(malware_data, 
                   columns=['classification', 'hash',
                            'signatures', 'behavior_calls', 'behavior_dll_loaded', 'network_udp_src', 'network_udp_dst', 'network_http', 'network_hosts', 'strings',
                            'embedded_domains_list', 'trid', 'pe_resource_list', 'imports_list', 'contacted_urls_list', 'type', 'tags_list'])

    ## Checking for large data
    #for i in range(len(malware_data)):
    #    for key in malware_data[i]:
    #        if len(malware_data[i][key]) > 32000:
    #            print(key + ' : ' + str(len(malware_data[i][key])))

    df.to_csv(path, index=None, header=True)

    #with xlsxwriter.Workbook(path, {'strings_to_urls': False}) as workbook:
    #    worksheet = workbook.add_worksheet();
    #    row = 0;
    #    col = 0;
    #    # Header Row
    #    worksheet.write(row, col, 'classification'); col += 1;
    #    worksheet.write(row, col, 'hash'); col += 1;

    #    worksheet.write(row, col, 'behavior_calls'); col += 1;
    #    worksheet.write(row, col, 'network_udp_src'); col += 1;
    #    worksheet.write(row, col, 'network_udp_dst'); col += 1;

    #    worksheet.write(row, col, 'embedded_domains_list'); col += 1;
    #    worksheet.write(row, col, 'trid'); col += 1;
    #    worksheet.write(row, col, 'pe_resource_list'); col += 1;
    #    worksheet.write(row, col, 'imports_list'); col += 1;
    #    worksheet.write(row, col, 'contacted_urls_list'); col += 1;
    #    worksheet.write(row, col, 'type'); col += 1;
    #    worksheet.write(row, col, 'tags_list'); col += 1;
    #    row += 1;
    #    # Data Rows
    #    for key in range(len(malware_data)):
    #        col = 0;
    #        worksheet.write(row, col, malware_data[key]['classification']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['hash']); col += 1;

    #        worksheet.write(row, col, malware_data[key]['behavior_calls']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['network_udp_src']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['network_udp_dst']); col += 1;

    #        worksheet.write(row, col, malware_data[key]['embedded_domains_list']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['trid']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['pe_resource_list']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['imports_list']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['contacted_urls_list']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['type']); col += 1;
    #        worksheet.write(row, col, malware_data[key]['tags_list']); col += 1;
    #        row += 1;
    return

def SplitData(malware_data, min_samples, max_samples):
    # Prune out anything with less than min_samples instances of a class
    print('Pruning dataset...')
    malware_class_dict = {}
    pruned_malware_class_dict = {}
    for key in malware_data:
        classification = malware_data[key]['classification']
        if classification in malware_class_dict:
            malware_class_dict[classification].append(malware_data[key])
        else:
            malware_class_dict[classification] = []
            malware_class_dict[classification].append(malware_data[key])

    for key in malware_class_dict:
        if len(malware_class_dict[key]) >= min_samples:
            pruned_malware_class_dict[key] = malware_class_dict[key]

    malware_data = []
    del malware_class_dict

    training_malware_data = []
    testing_malware_data = []
    validation_malware_data = []

    # Debug Purposes
    for key in sorted(pruned_malware_class_dict, key=lambda k: len(pruned_malware_class_dict[k]), reverse=True):
        print(key + ':' + str(len(pruned_malware_class_dict[key])))

    # Ensure at least one instance is in the training/test/validation set
    for key in pruned_malware_class_dict:
        random.shuffle(pruned_malware_class_dict[key])
        for i in range(0, 1):
            training_malware_data.append(pruned_malware_class_dict[key][i*3 + 0])
            testing_malware_data.append(pruned_malware_class_dict[key][i*3 + 1])
            validation_malware_data.append(pruned_malware_class_dict[key][i*3 + 2])
        for i in range(3, min(len(pruned_malware_class_dict[key]), max_samples)):
            malware_data.append(pruned_malware_class_dict[key][i])

    # Randomly Split Malware Data into Training/Test/Validation Sets
    print('Splitting data...')
    total_length = len(malware_data) + len(training_malware_data) + len(testing_malware_data) + len(validation_malware_data)
    training_size = int(total_length * (8 / 10))
    testing_size = int((total_length - training_size) / 2)
    validation_size = int((total_length - training_size) / 2)
    if testing_size < len(testing_malware_data):
        training_size = training_size - (len(testing_malware_data) - testing_size)
        testing_size = len(testing_malware_data)
    if validation_size < len(validation_malware_data):
        training_size = training_size - (len(testing_malware_data) - validation_size)
        validation_size = len(validation_malware_data)

    random.shuffle(malware_data)
    i = 0
    while len(training_malware_data) < training_size:
        training_malware_data.append(malware_data[i])
        i += 1

    while len(testing_malware_data) < testing_size:
        testing_malware_data.append(malware_data[i])
        i += 1

    while len(validation_malware_data) < validation_size:
        validation_malware_data.append(malware_data[i])
        i += 1

    return training_malware_data, testing_malware_data, validation_malware_data

if __name__ == '__main__':
    ## Get Command-line Arguments #################
    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--data', default='../../data', help='')
    opts = parser.parse_args()
    ###############################################

    PreProcessing(opts.data)